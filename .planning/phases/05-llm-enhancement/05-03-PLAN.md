---
phase: 05-llm-enhancement
plan: 03
type: execute
wave: 3
depends_on: ["05-02"]
files_modified:
  - src/commands/transpile.ts
  - src/cli.ts
autonomous: false

must_haves:
  truths:
    - User completes transpilation and is offered LLM enhancement pass
    - User can decline enhancement and continue to markdown export
    - User can accept enhancement and iterate until satisfied
    - User without API key sees local LLM fallback tips (Ollama, LM Studio, llama.cpp)
    - --no-enhance flag skips LLM phase entirely
  artifacts:
    - path: src/commands/transpile.ts
      provides: LLM enhancement hook after report generation
      contains: "detectAndConfirmAPIConfig"
      min_lines: 200
    - path: src/cli.ts
      provides: --no-enhance CLI flag
      contains: "--no-enhance"
  key_links:
    - from: src/commands/transpile.ts
      to: src/lib/llm/api-config.ts
      via: Calls detectAndConfirmAPIConfig after report
      pattern: "detectAndConfirmAPIConfig"
    - from: src/commands/transpile.ts
      to: src/lib/llm/llm-enhancer.ts
      via: Creates LLMEnhancer and runs enhancement
      pattern: "new LLMEnhancer|enhanceTranspilationResult"
---

<objective>
Integrate LLM enhancement into transpile command flow, offer enhancement pass after algorithmic report, handle no-key fallback gracefully, and provide --no-enhance flag for users who want to skip entirely.

Purpose: Complete the two-pass architecture (algorithmic + optional LLM) and deliver end-to-end LLM enhancement capability to users.

Output: Working LLM enhancement pass in transpile command with fallback messaging and CLI flag.
</objective>

<execution_context>
@C:\Users\Terence\code\gsd-for-hobos\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\Terence\code\gsd-for-hobos\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@C:\Users\Terence\code\gsd-for-hobos\.planning\PROJECT.md
@C:\Users\Terence\code\gsd-for-hobos\.planning\phases\05-llm-enhancement\05-CONTEXT.md
@C:\Users\Terence\code\gsd-for-hobos\.planning\phases\05-llm-enhancement\05-RESEARCH.md
@C:\Users\Terence\code\gsd-for-hobos\.planning\phases\05-llm-enhancement\05-01-SUMMARY.md
@C:\Users\Terence\code\gsd-for-hobos\.planning\phases\05-llm-enhancement\05-02-SUMMARY.md

# Existing command structure
@C:\Users\Terence\code\gsd-for-hobos\src\commands\transpile.ts
@C:\Users\Terence\code\gsd-for-hobos\src\cli.ts
@C:\Users\Terence\code\gsd-for-hobos\src\types\index.ts
</context>

<tasks>

<task type="auto">
  <name>Add LLM enhancement hook to transpile command</name>
  <files>src/commands/transpile.ts</files>
  <action>
Integrate LLM enhancement into transpile command flow after Step 3 (report generation), before Step 4 (markdown export):

**Add to TranspileCommandOptions interface:**
- noEnhance?: boolean (flag to skip LLM phase)

**New Step 3.5 - LLM Enhancement Pass (after report display, before markdown export):**

1. Check if --no-enhance flag is set: if true, skip to markdown export
2. Check if result.success is false: if true, skip enhancement (no point enhancing failed transpilation)
3. Check if options.dryRun or options.quiet: if true, skip enhancement (not appropriate for these modes)
4. Offer enhancement via confirm():
   - Message: "Enhance transpilation with LLM? (requires API key)"
   - initialValue: false (user must opt-in)
   - If isCancel() or user declines, skip to markdown export
5. If user accepts:
   - Import detectAndConfirmAPIConfig from ../lib/llm/api-config.js
   - Call apiConfig = await detectAndConfirmAPIConfig()
   - If apiConfig is null (user declined or all detection failed):
     - Display fallback message:
       - "No API key configured. You can still get enhanced reports by running a local LLM:"
       - Bullet list with links:
         - "Ollama: https://ollama.ai/docs/getting-started"
         - "LM Studio: https://lmstudio.ai/"
         - "llama.cpp: https://github.com/ggerganov/llama.cpp"
       - "Or use --no-enhance to skip this prompt next time."
     - Continue to markdown export (don't fail)
   - If apiConfig exists:
     - Import LLMEnhancer, DocsCacheManager from lib/llm
     - Create cacheManager = new DocsCacheManager()
     - Create enhancer = new LLMEnhancer(apiConfig, cacheManager)
     - Call enhancementResult = await enhancer.enhanceTranspilationResult(result)
     - If enhancementResult.success:
       - log.success(`Enhancement complete! Applied ${enhancementResult.appliedRules} new rules`)
       - Offer to regenerate report with new rules via confirm()
       - If yes, re-run transformer and reporter with llm-rules.json merged
     - If enhancementResult.errors.length > 0:
       - log.warn("Enhancement encountered errors:")
       - For each error, log.warn(error)
       - Don't fail the command - continue to markdown export
6. Continue to markdown export (existing Step 4)

**Error handling:**
- Wrap entire LLM enhancement section in try/catch
- On any error, log.warn() and continue to markdown export
- Never let LLM enhancement failure block transpilation success
- User should see: "LLM enhancement failed: {message}. Continuing with algorithmic result."

Import new dependencies:
- detectAndConfirmAPIConfig from ../lib/llm/api-config.js
- LLMEnhancer, DocsCacheManager from ../lib/llm/
- Keep existing imports

Follow established patterns:
- Use isCancel() for prompt handling
- Use log methods for user feedback
- Set process.exitCode appropriately (enhancement errors should warn, not fail)
  </action>
  <verify>npm run build succeeds, TypeScript compiles</verify>
  <done>Transpile command offers LLM enhancement after report, handles fallback gracefully</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
End-to-end LLM enhancement flow integrated into transpile command:
- API detection and testing (05-01)
- LLM calls with conversation history (05-02)
- Enhancement orchestration with validation (05-02)
- Command integration with fallback messaging (05-03)
  </what-built>
  <how-to-verify>
**Test 1: With valid API key**
1. Set environment variable: `export OPENAI_API_KEY=your-key`
2. Run: `npm run dev -- transpile` (assumes GSD is detected)
3. After algorithmic report displays, verify:
   - Prompt: "Enhance transpilation with LLM?"
   - Accept enhancement
   - Prompt: "Found OpenAI API key. Use it?"
   - Accept
   - Should see: "Testing endpoint..." → success
   - Should see: "What would you like to improve?" prompt
   - Enter a refinement request
   - Verify LLM response is validated and applied
   - Prompt: "Want to try more things?"
   - Test both continuing and exiting
4. Verify llm-rules.json created in .opencode/ with new rules

**Test 2: Without API key (fallback messaging)**
1. Unset all API key env vars
2. Run: `npm run dev -- transpile`
3. Accept enhancement prompt
4. Decline manual API key entry when prompted
5. Verify fallback message displays:
   - "No API key configured. You can still get enhanced reports by running a local LLM:"
   - Links to Ollama, LM Studio, llama.cpp
   - Mention of --no-enhance flag
6. Verify command continues to markdown export (doesn't fail)

**Test 3: With --no-enhance flag**
1. Run: `npm run dev -- transpile --no-enhance`
2. Verify enhancement prompt is skipped entirely
3. Verify command goes straight to markdown export

**Test 4: Invalid API key (graceful error)**
1. Set OPENAI_API_KEY to invalid value
2. Run enhancement flow
3. Verify endpoint test fails with clear message
4. Verify user is offered manual entry or fallback
5. Verify command doesn't crash

**Test 5: Malformed LLM output (validation)**
1. Run with valid API key
2. If LLM returns incomplete JSON, verify:
   - Validation catches the error
   - User sees validation error message
   - Retry is offered via conversation loop
3. Verify command doesn't crash on invalid output
  </how-to-verify>
  <resume-signal>
Type "approved" if all tests pass, or describe any issues encountered.
  </resume-signal>
</task>

</tasks>

<verification>
E2E verification checklist:
- [ ] Valid API key flow completes enhancement successfully
- [ ] No API key shows fallback message and continues gracefully
- [ ] --no-enhance flag skips enhancement entirely
- [ ] Invalid API key caught during endpoint test
- [ ] Malformed LLM output caught by schema validation
- [ ] Enhancement errors don't block transpilation success
- [ ] llm-rules.json created/updated correctly
- [ ] Conversation history accumulates across iterations
- [ ] User can exit enhancement loop at any point
- [ ] Command respects quiet/dry-run/no-enhance flags
</verification>

<success_criteria>
- User with OPENAI_API_KEY completes full enhancement loop (detection → testing → enhancement → iteration → exit)
- User without API key sees helpful fallback message with local LLM options
- User with --no-enhance flag skips enhancement prompt entirely
- Enhancement errors logged as warnings, don't fail transpilation
- llm-rules.json written with LLM-generated rules (separate from user rules)
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-enhancement/05-03-SUMMARY.md`
</output>
