---
phase: 05-llm-enhancement
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/lib/llm/cache-manager.ts
  - src/lib/llm/schema-validator.ts
  - src/lib/llm/llm-enhancer.ts
autonomous: true

must_haves:
  truths:
    - LLM can receive transpilation result and suggest improvements
    - User can iterate with LLM through multiple refinement rounds
    - LLM-generated rules are validated before applying
    - OpenCode docs are fetched once and cached for 24 hours
    - Invalid LLM output gets caught and user sees helpful error
  artifacts:
    - path: src/lib/llm/cache-manager.ts
      provides: GitHub docs caching with TTL
      exports: ["DocsCacheManager"]
      min_lines: 80
    - path: src/lib/llm/schema-validator.ts
      provides: JSON schema validation for LLM output
      exports: ["validateTransformRules", "ValidationResult"]
      min_lines: 60
    - path: src/lib/llm/llm-enhancer.ts
      provides: LLM enhancement orchestration
      exports: ["LLMEnhancer", "EnhancementResult"]
      min_lines: 200
  key_links:
    - from: src/lib/llm/cache-manager.ts
      to: "node:fs/promises"
      via: File-based TTL cache
      pattern: "stat|readFile|writeFile"
    - from: src/lib/llm/llm-enhancer.ts
      to: src/lib/llm/api-config.ts
      via: Uses APIConfig from detection
      pattern: "import.*APIConfig"
    - from: src/lib/llm/llm-enhancer.ts
      to: "node:fetch"
      via: Chat completions API calls
      pattern: "fetch.*chat/completions"
    - from: src/lib/llm/llm-enhancer.ts
      to: src/lib/llm/schema-validator.ts
      via: Validates LLM output before applying
      pattern: "validateTransformRules"
---

<objective>
Build LLM enhancement core: cache OpenCode docs from GitHub, call LLM APIs with conversation history, validate output, and orchestrate iterative refinement loop. This is the engine that powers the enhancement pass.

Purpose: Enable users to leverage LLM intelligence to improve transpilation quality beyond algorithmic transformation, with safeguards for malformed output and stale documentation.

Output: Complete LLM enhancement engine with docs caching, API calls, schema validation, and iteration loop.
</objective>

<execution_context>
@C:\Users\Terence\code\gsd-open\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\Terence\code\gsd-open\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@C:\Users\Terence\code\gsd-open\.planning\PROJECT.md
@C:\Users\Terence\code\gsd-open\.planning\phases\05-llm-enhancement\05-CONTEXT.md
@C:\Users\Terence\code\gsd-open\.planning\phases\05-llm-enhancement\05-RESEARCH.md
@C:\Users\Terence\code\gsd-open\.planning\phases\05-llm-enhancement\05-01-SUMMARY.md

# Types and existing modules
@C:\Users\Terence\code\gsd-open\src\types\index.ts
@C:\Users\Terence\code\gsd-open\src\lib\logger.ts
</context>

<tasks>

<task type="auto">
  <name>Create GitHub docs cache manager with TTL</name>
  <files>src/lib/llm/cache-manager.ts</files>
  <action>
Implement TTL-based caching following Research Pattern 2:

**DocsCacheManager class:**
- Constructor accepts cacheDir (default: '~/.cache/docs-opencode') and ttlSeconds (default: 86400 = 24 hours)
- get(key: string) method:
  - Check if cache file exists at {cacheDir}/{key}.cache
  - Use stat() to get file modification time (mtimeMs)
  - Calculate age: (Date.now() - mtimeMs) / 1000
  - If age > ttlSeconds, delete cache file and return null
  - Otherwise read and return cached content
  - Return null on any error (file not found, etc.)
- set(key: string, content: string) method:
  - Create cacheDir if it doesn't exist (recursive: true)
  - Write content to {cacheDir}/{key}.cache
  - Log verbose message about cache storage
- fetchOpenCodeDocs() method:
  - Try get('opencode-docs') first
  - If cached, log "Using cached OpenCode documentation" and return
  - If not cached, fetch from https://raw.githubusercontent.com/sst/opencode/main/docs/schema.md
  - Check response.ok, throw error if fetch fails
  - Read response text, call set('opencode-docs', content), return content

Use native node:fs/promises (readFile, writeFile, stat, unlink, mkdir) and node:path (join).

Import log from logger for verbose/info logging.
  </action>
  <verify>npm run build succeeds, TypeScript compiles</verify>
  <done>DocsCacheManager can fetch, cache, and retrieve OpenCode docs with 24-hour TTL</done>
</task>

<task type="auto">
  <name>Create schema validator for LLM-generated rules</name>
  <files>src/lib/llm/schema-validator.ts</files>
  <action>
Implement custom schema validation following Research Pattern 4:

**ValidationResult interface:**
- valid: boolean
- errors: string[]

**validateTransformRules(obj: unknown) function:**
1. Check obj is object (not null, not array): if not, return { valid: false, errors: ['Expected object'] }
2. Check obj has 'rules' property that is array: if not, return { valid: false, errors: ['Missing "rules" array'] }
3. For each rule in rules array:
   - Check rule is object (not null)
   - Check required fields exist and have correct types:
     - field: string
     - category: one of ['unsupported', 'platform', 'missing-dependency']
     - suggestion: string
   - Check optional fields if present:
     - example: string (if defined)
     - sourceFile: string (if defined)
   - Accumulate errors with descriptive messages like "Rule 0: field must be string"
4. Return { valid: errors.length === 0, errors }

DO NOT use zod or ajv - keep validation manual and lightweight for MVP.

Export validateTransformRules and ValidationResult.
  </action>
  <verify>npm run build succeeds, validation catches malformed rules</verify>
  <done>validateTransformRules can detect missing fields, wrong types, and invalid categories</done>
</task>

<task type="auto">
  <name>Build LLM enhancement orchestrator with conversation history</name>
  <files>src/lib/llm/llm-enhancer.ts</files>
  <action>
Create LLM enhancement orchestrator following Research Patterns 3 and 4:

**Interfaces:**
- ConversationMessage: { role: 'user' | 'assistant', content: string }
- EnhancementResult: { success: boolean, appliedRules: number, errors: string[] }
- ChatCompletionRequest/Response: Follow OpenAI chat completions API schema from research

**LLMEnhancer class:**

Constructor:
- Accepts apiConfig: APIConfig (from api-config module)
- Accepts cacheManager: DocsCacheManager
- Initializes conversationHistory: ConversationMessage[] = []

**enhanceTranspilationResult(result: TranspileResult) method:**
1. Load OpenCode docs via cacheManager.fetchOpenCodeDocs()
2. Build system prompt:
   - "You are an expert at transpiling GSD context engineering to OpenCode format."
   - "Review the algorithmic transpilation result and suggest improvements."
   - Include OpenCode schema docs for reference
   - Include current gaps from result.gaps
3. Enter iteration loop:
   - Call gatherUserRefinementRequest() - uses @clack/prompts text() to ask "What would you like to improve?"
   - Push user message to conversationHistory
   - Call callLLM() with system prompt and full conversationHistory
   - Push assistant response to conversationHistory
   - Parse response with parseEnhancementResponse() - expects JSON with { rules: [...] }
   - Validate with validateTransformRules()
   - If validation fails:
     - Push validation error feedback to conversationHistory: "Validation failed: {errors}. Please fix and try again."
     - Continue loop (retry)
   - If validation succeeds:
     - Call applyEnhancement() to merge rules into llm-rules.json
     - Display what changed with log.success()
   - Ask with confirm(): "Want to try more things?", initialValue: true
   - If user says no or isCancel(), exit loop
4. Return EnhancementResult with success, appliedRules count, errors

**callLLM(systemPrompt: string, messages: ConversationMessage[]) method:**
1. Build request body per OpenAI chat completions API:
   - model: apiConfig.model
   - messages: [{ role: 'system', content: systemPrompt }, ...messages]
   - temperature: 0.7
   - max_tokens: 2000
2. POST to {apiConfig.endpoint}/chat/completions with Authorization: Bearer {apiConfig.apiKey}
3. Parse response JSON, extract choices[0].message.content
4. Return content string
5. Handle errors: log verbose error details, throw with clear message

**applyEnhancement(rules: TransformRule[]) method:**
1. Load existing llm-rules.json if it exists (from .opencode/ or detected config dir)
2. Merge new rules with existing (simple append for MVP)
3. Sort by field name for deterministic output
4. Write back to llm-rules.json with JSON.stringify(obj, null, 2)
5. Log which rules were added

**parseEnhancementResponse(response: string) method:**
1. Try JSON.parse(response)
2. If parse fails, look for JSON block in markdown (```json ... ```)
3. If still fails, throw error with helpful message
4. Return parsed object

Import dependencies:
- APIConfig from ./api-config.js
- DocsCacheManager from ./cache-manager.js
- validateTransformRules from ./schema-validator.js
- TranspileResult from ../../types/index.js
- @clack/prompts: text, confirm, isCancel
- node:fs/promises: readFile, writeFile
- log from ../logger.js

Follow established ESM patterns (.js imports).
  </action>
  <verify>npm run build succeeds, TypeScript type checking passes</verify>
  <done>LLMEnhancer can orchestrate full enhancement loop with conversation history, validation, and rule application</done>
</task>

</tasks>

<verification>
Manual verification:
1. Create mock TranspileResult with gaps, pass to LLMEnhancer
2. Confirm LLM is called with system prompt including OpenCode docs
3. Confirm conversation history accumulates across iterations
4. Test validation catches malformed JSON from LLM
5. Confirm llm-rules.json is created/updated with new rules
</verification>

<success_criteria>
- OpenCode docs fetched once per session and cached (stat check shows cache file)
- LLM receives full conversation history (logged request body shows all messages)
- Malformed LLM output caught by validator before applying (validation error shown, retry offered)
- Valid LLM rules merged into llm-rules.json (file exists with new rules)
- User can iterate multiple times (loop continues until user exits)
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-enhancement/05-02-SUMMARY.md`
</output>
